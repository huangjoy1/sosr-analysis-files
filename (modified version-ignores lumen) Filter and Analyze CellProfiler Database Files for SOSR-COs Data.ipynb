{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c006fae7",
   "metadata": {},
   "source": [
    "# (modified version: ignores lumen) Filter and Analyze CellProfiler Database Files for SOSR-COs Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce1be1",
   "metadata": {},
   "source": [
    "Andrew Tidball Lab, University of Michigan School of Medicine Department of Neurology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a17f2b",
   "metadata": {},
   "source": [
    "**Notebook Summary**\n",
    "\n",
    "* reads in CellProfiler database file\n",
    "* Remove all columns that start with \"GFP_Core\"\n",
    "* reads in metadata and welldata\n",
    "* Remove organoids (each row is an organoid) that have NaN values for (organoid area)\n",
    "* merge in the metadata and welldata\n",
    "* one-way ANOVA and Kruskal test on the organoid area for each concentration, (did NOT filter for lumen number, and did NOT filter for area yet)\n",
    "* lists of (organoid area) for each concentration+condition (did NOT filter for lumen number, and did NOT filter for area yet) to excel file called (databaseFile)_Area_SOSRSonly.xlsx\n",
    "* Filter out the organoids based on organoid area\n",
    "* write the filtered file to an Excel file called (databaseFile)_filtered_SOSRSonly.xlsx. It is ready to be sent to the heatmap_final.ipynb Jupyter Notebook file to graph heatmaps and concentric maps\n",
    "* Center and scale the desired columns for all of the conditions. You can customize which columns are getting centered+scaled. Write the filtered+centeredScaled combined-conditions file to an Excel file called (databaseFile)_centeredScaled_combined_SOSRSonly.xlsx\n",
    "* Center and scale the desired columns for each unique condition. You can customize which columns are getting centered+scaled. Write the filtered+centeredScaled file for each unique condition to separate Excel files called (databaseFile)\\_centeredScaled\\_(conditionName)_SOSRSonly.xlsx\n",
    "\n",
    "**How to Use**\n",
    "* edit the cell named \"User Input Edit Required\"\n",
    "* Kernel -> Restart and Run All\n",
    "\n",
    "**Result**\n",
    "* 4 types of excel files to a specified folder\n",
    "* (databaseFile)_Area_SOSRSonly.xlsx\n",
    "* (databaseFile)_filtered_SOSRSonly.xlsx\n",
    "* (databaseFile)_centeredScaled_combined_SOSRSonly.xlsx\n",
    "* (databaseFile)\\_centeredScaled\\_(conditionName)_SOSRSonly.xlsx (we will have N of these files, where N is the number of unique conditions we have)\n",
    "\n",
    "**Note**\n",
    "* this file is **a modified copy of the \"Filter and Analyze - original\" Jupyter Notebook file**. In this file, we remove all the GFP_Core columns, don't do any filtering and analysis based on (number of lumens), and do not calculate for the area ratio\n",
    "* there are some old database files that did not have MyExpt_Per_object table in the SQL database files. this Jupyter notebook file does not support those database files\n",
    "* this Jupyter notebook file does not support database files that have multiple plates all combined in one file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8031e65",
   "metadata": {},
   "source": [
    "**Import libraries**: here, we import Python libraries that allow us to efficiently run statistical analysis, plot graphs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import scipy\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1421369",
   "metadata": {},
   "source": [
    "**User Input Edit Required:**\n",
    "* set the **databaseFile** variable to the name of the database file you want to read from\n",
    "* set the **folder** variable to the name of the folder that you want to save the excel files to\n",
    "* set the **metadataFile** variable to the name of the metadata file you want to read from\n",
    "* set the **metadataSheet** variable to the name of the specific sheet of the metadataFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##USER INPUT##\n",
    "databaseFile = \"SLFA-093\"\n",
    "folder = \"Results_SOSRSonly\"\n",
    "metadataFile = \"Well_vs_Cond - 96 well plate SLFA93.xlsx\"\n",
    "metadataSheet = \"Well_vs_Cond - 96 well plate sc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d15efb",
   "metadata": {},
   "source": [
    "**Retrieve the database SQL file**:\n",
    "1. Next, we connect to the databaseFile's SQL database using the SQLAlchemy library by creating an engine called cnx\n",
    "2. We use panda's read_sql_table function to read from the cnx engine. We are reading in the \"MyExpt_Per_Object\" table, and storing it in a pandas dataframe called **df**\n",
    "* uncomment display(df) to get an overview of what df currently looks like\n",
    "* uncomment df.to_excel(\"Raw.xlsx\") to get an excel sheet of the original database file\n",
    "* uncomment the last two lines to get a list of available table names in the SQL engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1daaf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy connectable\n",
    "cnx = create_engine('sqlite:///' + databaseFile + \".db\").connect()\n",
    "\n",
    "# table will be returned as a dataframe.\n",
    "df = pd.read_sql_table('MyExpt_Per_Object', cnx)\n",
    "\n",
    "#display(df)\n",
    "#df.to_excel(\"Raw.xlsx\")\n",
    "\n",
    "##uncomment below if you want a list of table names\n",
    "#table_list = [a for a in cnx.execute(\"SELECT name FROM sqlite_master WHERE type = 'table'\")]\n",
    "#print(table_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc6ef9",
   "metadata": {},
   "source": [
    "**Remove all columns that start with \"GFP_Core\"**\n",
    "1. store all columns with names that include 'GFP_Core' in **gfp_cols** list\n",
    "2. In **df**, drop the columns mentioned in **gfp_cols**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp_cols = df.columns[df.columns.str.contains(pat='.*GFP_Core.*')].tolist()\n",
    "display(gfp_cols)\n",
    "df = df.drop(columns=gfp_cols)\n",
    "display(df)\n",
    "\n",
    "for c in df.columns:\n",
    "    print(c, df[c].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b308180",
   "metadata": {},
   "source": [
    "**Read in metadata and well data**:\n",
    "1. Use panda's read_excel function to read in the metadata file. If there are multiple sheet tabs, specify which sheet tab you want to read in \"sheet_name = \". It reads in the file and stores it in a panda's dataframe called **metadata**\n",
    "2. Next, we use panda's read_sql_table function to read from the cnx engine. We are reading in the \"MyExpt_Per_Image\" table, and storing it in a pandas dataframe called **well**\n",
    "3. well is a big file, but we are only interested in two of its columns. Here, we are extracting out the \"ImageNumber\" and \"Image_Metadata_WellID\" columns. Now, the **well** dataframe just has these two columns.\n",
    "* note: can use display(well) or display(metadata) to get an overview of those dataframes\n",
    "* uncomment well.to_excel(\"Well.xlsx\") to get an excel sheet of the well data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e2884",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_excel(metadataFile, sheet_name = metadataSheet)\n",
    "\n",
    "well = pd.read_sql_table('MyExpt_Per_Image', cnx)\n",
    "\n",
    "well = well[[\"ImageNumber\", \"Image_Metadata_WellID\"]]\n",
    "\n",
    "display(metadata)\n",
    "display(well)\n",
    "#well.to_excel(\"Well.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96141186",
   "metadata": {},
   "source": [
    "**Remove organoids that have NaN values for (organoid area)**\n",
    "1. We don't want the NaN values for (organoid area). In **df**, we drop a row if its  'spheroids_AreaShape_Area' is NaN.\n",
    "\n",
    "**df** now has:\n",
    "* filtered out NaN values for (organoid area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prints out all column names and their value type \n",
    "#for c in df.columns:\n",
    "#    print(c, df[c].dtype)\n",
    "    \n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "\n",
    "df = df.dropna(subset=['spheroids_AreaShape_Area'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9a6ad",
   "metadata": {},
   "source": [
    "**Adding the metadata and well information**\n",
    "1. Use panda's merge function to merge two dataframes: **df** and **well**. Their shared column is \"ImageNumber\" \n",
    "2. **df** now has a new column called 'Image_Metadata_WellID' at the end. We are renaming that column to 'WellID'\n",
    "3. Use panda's merge function to merge two dataframes: **df** and **metadata**. Their shared column is \"WellID\" \n",
    "4. note: if you want to export the current **df** dataframe to excel, use df.to_excel(\"after_merge.xlsx\"). you can change the file name\n",
    "5. note: panda's merge can merge two dataframes in different ways. It's important to understand how it's merging and if the function is actually doing what you hope to do. https://www.digitalocean.com/community/tutorials/pandas-merge-two-dataframe\n",
    "\n",
    "**df** now has (after running the cell below):\n",
    "* filtered out NaN values for (organoid area)\n",
    "* added 7 new columns by merging with well and metadata: WellID, WellNo, Row, Column, Condition, Concentration, Dye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,well, on='ImageNumber')\n",
    "df.rename(columns = {'Image_Metadata_WellID':'WellID'}, inplace = True)\n",
    "df = pd.merge(df, metadata, on='WellID')\n",
    "#df.to_excel(\"after_merge.xlsx\")\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ddbc16",
   "metadata": {},
   "source": [
    "**one-way ANOVA and Kruskal test on the organoid area for each condition+concentration combination, (did not filter for (number of lumen). did not filter for (spheroid area) yet)**\n",
    "1. using the **df** dataframe, make separate lists of (organoid area) for each condition+concentration combination, and store them to the variable **areaK**. The best way to understand what areaK is storing is to look at it with display(areaK)\n",
    "2. use scipy library's one way ANOVA function on **areaK**. we are storing the results to a variable called **oneAnova**. \n",
    "3. call display(oneAnova) to see the ANOVA p-value\n",
    "2. use scipy library's kruskal function on **areaK**. we are storing the results to a variable called **kruskal**\n",
    "3. use display(kruskal) to look at the kruskal p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "areaK = df.groupby(['Condition', 'Concentration'])['spheroids_AreaShape_Area'].apply(list)\n",
    "display(areaK)\n",
    "\n",
    "oneAnova = scipy.stats.f_oneway(*areaK)\n",
    "display(oneAnova)\n",
    "\n",
    "kruskal = scipy.stats.kruskal(*areaK)\n",
    "display(kruskal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23208dbd",
   "metadata": {},
   "source": [
    "**Writing lists of (organoid area) for each condition+concentration (did NOT filter for lumen number, and did NOT filter for area yet) to Excel**\n",
    "\n",
    "1. get a list of row labels from areaK. store it to an array variable called **row_labels**. One item in **row_labels** contains ('condition','concentration')\n",
    "2. create a new pandas dataframe called **df_anova**. This will be used to store the **df's** (organoid area) list categorized by condition+concentration\n",
    "3. for loop. x is the first item in **row_labels**. In **df**, find rows where the (Condition = x's condition) AND (concentration = x's concentration). For those rows, we get their (organoid area). Store that list of (organoid area) values into a list called **nrow**. concatenate **nrow** to **df_anova**. make x be the second item in **row_labels**. repeat the process until we get through the whole row_labels list\n",
    "4. set the column names of **df_anova** with row_labels\n",
    "\n",
    "**writing them to specific excel sheet tabs to an excel file called (databaseFile)_Area_SOSRSonly.xlsx **\n",
    "\n",
    "5. write df_anova dataframe to an excel sheet named '(databaseFile)_Area_SOSRSonly.xlsx', specifically to a tab named \"area\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_labels = areaK.index.values\n",
    "print(row_labels)\n",
    "print(row_labels.size)\n",
    "\n",
    "df_anova = pd.DataFrame()\n",
    "\n",
    "for x in row_labels:\n",
    "    nrow = df.loc[(df['Condition']==x[0]) & (df['Concentration']==x[1])].spheroids_AreaShape_Area\n",
    "    nrow.reset_index(drop=True, inplace=True)\n",
    "    df_anova = pd.concat([df_anova, nrow], axis=1, ignore_index=True)\n",
    "df_anova.columns = row_labels\n",
    "\n",
    "with pd.ExcelWriter(folder + \"/\"+databaseFile + '_Area_SOSRSonly.xlsx', engine='xlsxwriter') as writer:\n",
    "    df_anova.to_excel(writer, sheet_name='area', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf81d58",
   "metadata": {},
   "source": [
    "**Filter out the data based on organoid area**\n",
    "1. make a new dataframe called **df_filter**. copy **df** to **df_filter**\n",
    "2. using **df_filter**, select rows in which the \"spheroids_AreaShape_Area\" column is smaller than or equal to a number. update **df_filter**\n",
    "3. using **df_filter**, select rows in which the \"spheroids_AreaShape_Area\" column is greater than or equal to a number. update **df_filter**\n",
    "\n",
    "\n",
    "**df_filter** now has (after running the cell below):\n",
    "* filtered out NaN values for (organoid area)\n",
    "* added 7 new columns by merging with well and metadata: WellID, WellNo, Row, Column, Condition, Concentration, Dye\n",
    "* selected rows in which (organoid area) is in a desired range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad508e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter = df\n",
    "df_filter = df_filter.loc[df_filter['spheroids_AreaShape_Area'] <= 31415]\n",
    "df_filter = df_filter.loc[df_filter['spheroids_AreaShape_Area'] >= 7853]\n",
    "display(df_filter)                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f0970",
   "metadata": {},
   "source": [
    "**write the filtered file to an excel file: (databaseFile)_filtered_SOSRSonly.xlsx**\n",
    "\n",
    "This file is ready to be sent to the \"heatmap_final\" Jupyter Notebook file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9677c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter.to_excel(folder + \"/\"+ databaseFile + \"_filtered_SOSRSonly.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2042a8",
   "metadata": {},
   "source": [
    "**create a list of columns that you want to run center/scale on**\n",
    "1. get a list of all the column names in **df_filter**, store that list into **numList**\n",
    "2. **dropList** has a list of column names that you **DO NOT** want to run center/scale on\n",
    "3. for loop. go through each item in **dropList**, and remove that item from **numList**\n",
    "4. print out the final **numList**. This list contains all the column names that you want to run center/scale on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "numList = df_filter.columns.values.tolist()\n",
    "# remove additional columns that we don't want\n",
    "\n",
    "dropList = ['ImageNumber','ObjectNumber','spheroids_Number_Object_Number',\n",
    "          'WellID','WellNo','Row','Column','Condition','Concentration','Dye']\n",
    "\n",
    "for c in dropList:\n",
    "    numList.remove(c)\n",
    "\n",
    "print(numList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4602d486",
   "metadata": {},
   "source": [
    "**Center/scale the data (for all the conditions/concentration groups) and writing it to (databaseFile)_centeredScaled_combined_SOSRSonly.xlsx**\n",
    "1. make a new dataframe called **df_num**. copy **df** to **df_num**\n",
    "2. create an object of the StandardScaler() function called **scaler**\n",
    "3. run sklearn's fit_transform() function on **df_num**, but only on the columns mentioned in **numList**\n",
    "4. write **df_num** to an excel file called (databaseFile)_centeredScaled_combined_SOSRSonly.xlsx\n",
    "\n",
    "* we are using the sklearn.preprocessing.StandardScaler function\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df_filter\n",
    "scaler = StandardScaler()\n",
    "df_num[numList] = scaler.fit_transform(df_num[numList])\n",
    "df_num.to_excel(folder + \"/\"+databaseFile + \"_centeredScaled_combined_SOSRSonly.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ab76d",
   "metadata": {},
   "source": [
    "**center and scale all the columns (separate processing for each condition) and writing to one excel file for each condition called (databaseFile)\\_centeredScaled\\_(conditionName)_SOSRSonly.xlsx**\n",
    "1. create a StandardScaler() object called scaler\n",
    "2. get a list of unique conditions by running .unique() on the \"Conditions\" column. store this list to **conditionList**\n",
    "\n",
    "3. for loop. make variable **x** be the first item in **conditionList**. \n",
    "* make df_condCenter a new pandas dataframe. df_condCenter becomes a new empty dataframe very time the loop runs. \n",
    "* Set **df_condCenter** to **df_filter**, but only the rows where the Condition column is equal to **x**. \n",
    "* Run sklearn's fit_transform() function on df_condCenter, but only on the columns mentioned in **numList**. \n",
    "* Write **df_condCenter** to an excel file named (databaseFile)\\_centeredScaled\\_(conditionName)_SOSRSonly.xlsx\n",
    "* we finished one round of the for loop. **x** becomes the second item in **conditionList**, and we repeat the commands in the for loop. This continues until all of the items in **conditionList** has been looped through\n",
    "\n",
    "\n",
    "we are using the sklearn.preprocessing.StandardScaler function\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "conditionList= df_filter['Condition'].unique()\n",
    "print(conditionList)\n",
    "\n",
    "#df_combinedCenter = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for x in conditionList:\n",
    "    df_condCenter = pd.DataFrame()\n",
    "    df_condCenter = df_filter.loc[(df_filter['Condition']==x)]\n",
    "    df_condCenter[numList] = scaler.fit_transform(df_condCenter[numList])\n",
    "    display(df_condCenter)\n",
    "    df_condCenter.to_excel(folder + \"/\"+ databaseFile + \"_centeredScaled_\" + x + \"_SOSRSonly.xlsx\", index=False)\n",
    "    #df_combinedCenter = pd.concat([df_combinedCenter, df_condCenter], axis=0, ignore_index=True)\n",
    "\n",
    "#df_combinedCenter.to_excel(folder + \"/\"+ databaseFile + \"_centeredScaled_\" + \"combined\" + \".xlsx\", index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6b78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
